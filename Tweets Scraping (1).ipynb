{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "api_key = \"9oIEO3KzxEzqaOtrq0GSl02sJ\"\n",
    "api_sec_key = \"eYKWDjjDW5d2e2o7hNSGQjcTlotaVnHSgc4I4wx3p5NGUOi5Ak\"\n",
    "access_token = \"1409815353262104581-NWGFJBfNDuM7vji7fyfur2rZlsgCBj\"\n",
    "access_token_secret = \"N5R1enqxfxBteQ9cUb2IHlG1srrJcXOnORT0Cmmg2767e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentications\n",
    "auth = tweepy.OAuthHandler(api_key, api_sec_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'BBCBreaking'\n",
    "count = 10000\n",
    "\n",
    "# Creation of query method using parameters\n",
    "tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
    "# Pulling information from tweets iterable object\n",
    "tweets_list = [[tweet.created_at,tweet.text] for tweet in tweets]                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of dataframe from tweets list\n",
    "# Add or remove columns as you remove tweet information\n",
    "tweets_df = pd.DataFrame(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-30 19:33:34</td>\n",
       "      <td>Former US defence secretary Donald Rumsfeld, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30 17:04:50</td>\n",
       "      <td>US comedian Bill Cosby's sex assault convictio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29 17:51:47</td>\n",
       "      <td>RT @BBCSport: ENGLAND HAVE DONE IT! ðŸ”¥\\n\\n#ENG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-29 13:08:13</td>\n",
       "      <td>Foreign business leaders no longer need to qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-29 09:51:30</td>\n",
       "      <td>South Africa's former President Jacob Zuma sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>2018-03-15 14:45:24</td>\n",
       "      <td>US places sanctions on Russians accused of med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>2018-03-15 13:04:14</td>\n",
       "      <td>Ex-spy poisoning first offensive use of nerve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>2018-03-15 12:11:46</td>\n",
       "      <td>Neville Hord jailed for at least 30 years for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>2018-03-15 10:45:58</td>\n",
       "      <td>Man pleads guilty to murdering Aldi worker Jod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>2018-03-15 09:59:24</td>\n",
       "      <td>Russia will soon expel UK diplomats in retalia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                                                  1\n",
       "0    2021-06-30 19:33:34  Former US defence secretary Donald Rumsfeld, o...\n",
       "1    2021-06-30 17:04:50  US comedian Bill Cosby's sex assault convictio...\n",
       "2    2021-06-29 17:51:47  RT @BBCSport: ENGLAND HAVE DONE IT! ðŸ”¥\\n\\n#ENG ...\n",
       "3    2021-06-29 13:08:13  Foreign business leaders no longer need to qua...\n",
       "4    2021-06-29 09:51:30  South Africa's former President Jacob Zuma sen...\n",
       "...                  ...                                                ...\n",
       "3245 2018-03-15 14:45:24  US places sanctions on Russians accused of med...\n",
       "3246 2018-03-15 13:04:14  Ex-spy poisoning first offensive use of nerve ...\n",
       "3247 2018-03-15 12:11:46  Neville Hord jailed for at least 30 years for ...\n",
       "3248 2018-03-15 10:45:58  Man pleads guilty to murdering Aldi worker Jod...\n",
       "3249 2018-03-15 09:59:24  Russia will soon expel UK diplomats in retalia...\n",
       "\n",
       "[3250 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = tweets_df.rename(columns={0:'Published date',1:'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published date</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-30 19:33:34</td>\n",
       "      <td>Former US defence secretary Donald Rumsfeld, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-30 17:04:50</td>\n",
       "      <td>US comedian Bill Cosby's sex assault convictio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-29 17:51:47</td>\n",
       "      <td>RT @BBCSport: ENGLAND HAVE DONE IT! ðŸ”¥\\n\\n#ENG ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-29 13:08:13</td>\n",
       "      <td>Foreign business leaders no longer need to qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-29 09:51:30</td>\n",
       "      <td>South Africa's former President Jacob Zuma sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>2018-03-15 14:45:24</td>\n",
       "      <td>US places sanctions on Russians accused of med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>2018-03-15 13:04:14</td>\n",
       "      <td>Ex-spy poisoning first offensive use of nerve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>2018-03-15 12:11:46</td>\n",
       "      <td>Neville Hord jailed for at least 30 years for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>2018-03-15 10:45:58</td>\n",
       "      <td>Man pleads guilty to murdering Aldi worker Jod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>2018-03-15 09:59:24</td>\n",
       "      <td>Russia will soon expel UK diplomats in retalia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3250 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Published date                                              tweet\n",
       "0    2021-06-30 19:33:34  Former US defence secretary Donald Rumsfeld, o...\n",
       "1    2021-06-30 17:04:50  US comedian Bill Cosby's sex assault convictio...\n",
       "2    2021-06-29 17:51:47  RT @BBCSport: ENGLAND HAVE DONE IT! ðŸ”¥\\n\\n#ENG ...\n",
       "3    2021-06-29 13:08:13  Foreign business leaders no longer need to qua...\n",
       "4    2021-06-29 09:51:30  South Africa's former President Jacob Zuma sen...\n",
       "...                  ...                                                ...\n",
       "3245 2018-03-15 14:45:24  US places sanctions on Russians accused of med...\n",
       "3246 2018-03-15 13:04:14  Ex-spy poisoning first offensive use of nerve ...\n",
       "3247 2018-03-15 12:11:46  Neville Hord jailed for at least 30 years for ...\n",
       "3248 2018-03-15 10:45:58  Man pleads guilty to murdering Aldi worker Jod...\n",
       "3249 2018-03-15 09:59:24  Russia will soon expel UK diplomats in retalia...\n",
       "\n",
       "[3250 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[Timestamp('2021-06-30 19:33:34'),\n",
       "        'Former US defence secretary Donald Rumsfeld, one of the main architects of the Iraq war, dies at 88 https://t.co/N4jvaDkaPi'],\n",
       "       [Timestamp('2021-06-30 17:04:50'),\n",
       "        \"US comedian Bill Cosby's sex assault conviction overturned by top Pennsylvania court, paving the way for his release https://t.co/9JFWX1X66o\"],\n",
       "       [Timestamp('2021-06-29 17:51:47'),\n",
       "        \"RT @BBCSport: ENGLAND HAVE DONE IT! ðŸ”¥\\n\\n#ENG 2-0 #GER\\n\\nRaheem Sterling and Harry Kane's goals secure England's place in the quarter-finals oâ€¦\"],\n",
       "       ...,\n",
       "       [Timestamp('2018-03-15 12:11:46'),\n",
       "        'Neville Hord jailed for at least 30 years for stabbing to death Jodie Willsher, who was working at an Aldi supermarâ€¦ https://t.co/tULGnG9vin'],\n",
       "       [Timestamp('2018-03-15 10:45:58'),\n",
       "        'Man pleads guilty to murdering Aldi worker Jodie Willsher, who was stabbed in Yorkshire store in December https://t.co/M43VnIUu3Z'],\n",
       "       [Timestamp('2018-03-15 09:59:24'),\n",
       "        \"Russia will soon expel UK diplomats in retaliation for Britain's response to poisoning of ex-spy - Russian media https://t.co/7zOz2DkNeN\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.to_csv(\"BBCBreaking data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic modeling\n",
    "import nltk,gensim\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessed_text(document):\n",
    "  # Removing all the special character\n",
    "  document = re.sub(r'\\W',' ',str(document))\n",
    "  # Removing all single character\n",
    "  document = re.sub(r'\\s+[a-zA-Z]\\s+',' ', document)\n",
    "  # Removing single character from the start\n",
    "  document = re.sub(r'\\^[a-zA-Z]\\s+',' ',document)\n",
    "  # substituting multiple spaces with single spaces\n",
    "  document = re.sub(r'\\s+',' ',document,flags=re.I)\n",
    "  # Removing prefixed \n",
    "  document = re.sub(r'^b\\s+', '' , document)\n",
    "  # Converting to lowercase\n",
    "  document = document.lower()\n",
    "  # Lemmentization\n",
    "  tokens = document.split()\n",
    "  tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "  tokens = [word for word in tokens if word not in en_stop]\n",
    "  tokens = [word for word in tokens if len(word)>5]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "preprocessed_data = []\n",
    "for docu in numpy_array:\n",
    "  tokens = preprocessed_text(docu)\n",
    "  preprocessed_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['timestamp',\n",
       "  'former',\n",
       "  'defence',\n",
       "  'secretary',\n",
       "  'donald',\n",
       "  'rumsfeld',\n",
       "  'architect',\n",
       "  'n4jvadkapi'],\n",
       " ['timestamp',\n",
       "  'comedian',\n",
       "  'assault',\n",
       "  'conviction',\n",
       "  'overturned',\n",
       "  'pennsylvania',\n",
       "  'paving',\n",
       "  'release',\n",
       "  '9jfwx1x66o'],\n",
       " ['timestamp',\n",
       "  'bbcsport',\n",
       "  'england',\n",
       "  'nraheem',\n",
       "  'sterling',\n",
       "  'secure',\n",
       "  'england',\n",
       "  'quarter'],\n",
       " ['timestamp',\n",
       "  'foreign',\n",
       "  'business',\n",
       "  'leader',\n",
       "  'longer',\n",
       "  'quarantine',\n",
       "  'arriving',\n",
       "  'england',\n",
       "  'likely',\n",
       "  'significant',\n",
       "  '8oqcsrkyac'],\n",
       " ['timestamp',\n",
       "  'africa',\n",
       "  'former',\n",
       "  'president',\n",
       "  'sentenced',\n",
       "  'prison',\n",
       "  'highest',\n",
       "  'country',\n",
       "  'vbvk5txm6t'],\n",
       " ['timestamp',\n",
       "  'combination',\n",
       "  'pfizer',\n",
       "  'astrazeneca',\n",
       "  'vaccine',\n",
       "  'protection',\n",
       "  'coronavirus',\n",
       "  '18ldt2nfe3'],\n",
       " ['timestamp',\n",
       "  'classified',\n",
       "  'government',\n",
       "  'document',\n",
       "  'containing',\n",
       "  'detail',\n",
       "  'warship',\n",
       "  'defender',\n",
       "  'british',\n",
       "  'military',\n",
       "  'afghanist',\n",
       "  'v0qyhjgwnl'],\n",
       " ['timestamp',\n",
       "  'former',\n",
       "  'chancellor',\n",
       "  'secretary',\n",
       "  'replace',\n",
       "  'hancock',\n",
       "  'health',\n",
       "  'secretary',\n",
       "  'ww1p1eaff3'],\n",
       " ['timestamp', 'bbcsport', 'nwales', 'outplayed', 'denmark', 'heading'],\n",
       " ['timestamp', 'resign', 'posted', 'nygzng13no']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import  corpora\n",
    "\n",
    "input_dict = corpora.Dictionary(processed_data)\n",
    "input_doc = [input_dict.doc2bow(token,allow_update = True) for token in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comedian'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dict.get(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1)],\n",
       " [(7, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1)],\n",
       " [(7, 1),\n",
       "  (17, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1)],\n",
       " [(3, 1),\n",
       "  (7, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1)],\n",
       " [(7, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)],\n",
       " [(7, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1)],\n",
       " [(3, 1), (6, 2), (7, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1)],\n",
       " [(7, 1), (16, 1), (61, 1), (62, 1), (63, 1), (64, 1)],\n",
       " [(7, 1), (65, 1), (66, 1), (67, 1)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_doc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model-1 \n",
    "lda_model = gensim.models.ldamodel.LdaModel(input_doc,num_topics=10,id2word=input_dict,passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.101*\"timestamp\" + 0.040*\"bbcsport\" + 0.011*\"champion\" + 0.009*\"league\" + 0.008*\"nhttps\" + 0.007*\"england\" + 0.006*\"missing\" + 0.006*\"manager\" + 0.006*\"germany\" + 0.005*\"hospital\"'),\n",
       " (1,\n",
       "  '0.076*\"timestamp\" + 0.013*\"guilty\" + 0.011*\"vaccine\" + 0.010*\"killed\" + 0.008*\"israeli\" + 0.008*\"crashed\" + 0.007*\"report\" + 0.006*\"people\" + 0.005*\"israel\" + 0.005*\"oxford\"'),\n",
       " (2,\n",
       "  '0.092*\"timestamp\" + 0.019*\"president\" + 0.014*\"donald\" + 0.010*\"supreme\" + 0.008*\"coronavirus\" + 0.008*\"johnson\" + 0.008*\"former\" + 0.007*\"bbcsport\" + 0.006*\"england\" + 0.005*\"inquiry\"'),\n",
       " (3,\n",
       "  '0.107*\"timestamp\" + 0.012*\"brexit\" + 0.011*\"secretary\" + 0.010*\"theresa\" + 0.008*\"nhttps\" + 0.008*\"president\" + 0.007*\"labour\" + 0.006*\"coronavirus\" + 0.006*\"leader\" + 0.006*\"election\"'),\n",
       " (4,\n",
       "  '0.088*\"timestamp\" + 0.011*\"ireland\" + 0.010*\"northern\" + 0.007*\"chemical\" + 0.007*\"london\" + 0.007*\"official\" + 0.006*\"teenager\" + 0.006*\"murder\" + 0.005*\"andrew\" + 0.005*\"explosion\"'),\n",
       " (5,\n",
       "  '0.089*\"timestamp\" + 0.049*\"police\" + 0.034*\"people\" + 0.029*\"killed\" + 0.016*\"murder\" + 0.015*\"attack\" + 0.014*\"injured\" + 0.012*\"officer\" + 0.010*\"london\" + 0.010*\"report\"'),\n",
       " (6,\n",
       "  '0.095*\"timestamp\" + 0.022*\"president\" + 0.016*\"leader\" + 0.010*\"people\" + 0.008*\"government\" + 0.008*\"donald\" + 0.007*\"official\" + 0.007*\"lockdown\" + 0.007*\"million\" + 0.006*\"figure\"'),\n",
       " (7,\n",
       "  '0.073*\"timestamp\" + 0.016*\"police\" + 0.012*\"minister\" + 0.009*\"attack\" + 0.007*\"terror\" + 0.006*\"florida\" + 0.005*\"multiple\" + 0.005*\"strike\" + 0.005*\"shooting\" + 0.004*\"westminster\"'),\n",
       " (8,\n",
       "  '0.100*\"timestamp\" + 0.008*\"government\" + 0.008*\"brexit\" + 0.007*\"police\" + 0.006*\"nhttps\" + 0.006*\"election\" + 0.005*\"coronavirus\" + 0.005*\"people\" + 0.005*\"president\" + 0.004*\"manchester\"'),\n",
       " (9,\n",
       "  '0.029*\"timestamp\" + 0.008*\"partner\" + 0.007*\"suspicious\" + 0.007*\"package\" + 0.006*\"believe\" + 0.005*\"landmark\" + 0.005*\"hillsborough\" + 0.004*\"victim\" + 0.004*\"leaving\" + 0.004*\"rapper\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_topics=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Admin\\anaconda3\\python.exe' 'C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' build_wheel 'C:\\Users\\Admin\\AppData\\Local\\Temp\\tmph4s6be9c'\n",
      "       cwd: C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-34cqaidw\\hdbscan\n",
      "  Complete output (27 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.8\n",
      "  creating build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-3.8\\hdbscan\n",
      "  creating build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-3.8\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  skipping 'hdbscan\\_hdbscan_tree.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_linkage.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_boruvka.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_hdbscan_reachability.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\_prediction_utils.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan\\dist_metrics.c' Cython extension (up-to-date)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.8.1-py2.py3-none-any.whl (53 kB)\n",
      "Collecting pandas>=1.1.5\n",
      "  Using cached pandas-1.2.5-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "Collecting plotly<4.14.3,>=4.7.0\n",
      "  Using cached plotly-4.14.2-py2.py3-none-any.whl (13.2 MB)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.21.0-cp38-cp38-win_amd64.whl (14.0 MB)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "Collecting hdbscan>=0.8.27\n",
      "  Using cached hdbscan-0.8.27.tar.gz (6.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bertopic) (4.50.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from bertopic) (0.23.2)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap-learn-0.5.1.tar.gz (80 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2020.1)\n",
      "Collecting retrying>=1.3.3\n",
      "  Using cached retrying-1.3.3.tar.gz (10 kB)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\anaconda3\\lib\\site-packages (from plotly<4.14.3,>=4.7.0->bertopic) (1.15.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-win_amd64.whl (222.0 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp38-cp38-win_amd64.whl (920 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.5)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Collecting joblib>=1.0\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.27->bertopic) (0.29.21)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.51.2)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.2.tar.gz (1.1 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (5.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (20.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2020.10.15)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.34.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2020.6.20)\n",
      "Building wheels for collected packages: sentence-transformers, hdbscan, umap-learn, retrying, pynndescent\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.0.0-py3-none-any.whl size=126713 sha256=97b6b62093e983cff10ec55bb9c3c3b86c6cb7407c90c66e28dae8804a597573\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\8c\\b7\\50\\451c9a52a337aac5521dbc10544a69e1447d28012feba30742\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'error'\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76570 sha256=b30b44bf4bfbcc240958d77786ea485d4fd9440edb5716f3351dfc4eaaf01ce2\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\95\\85\\b7\\b4b7040e49367b6d1505d7e8fb57e3e79b22fa6ac26f72520b\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11434 sha256=f0c6ce868ac88d3b4156e4b46af977e605547c98714a8338e42d7135c55eb7f9\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\c4\\a7\\48\\0a434133f6d56e878ca511c0e6c38326907c0792f67b476e56\n",
      "  Building wheel for pynndescent (setup.py): started\n",
      "  Building wheel for pynndescent (setup.py): finished with status 'done'\n",
      "  Created wheel for pynndescent: filename=pynndescent-0.5.2-py3-none-any.whl size=51353 sha256=ea302530fd2d2c571a6914f72e25c7a83e400cf58ac1e639b7c070eb788ae364\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\bb\\81\\5d\\c78c8f15f3c815197129f91ec435d8ae0bb0a7a856d962c3ce\n",
      "Successfully built sentence-transformers umap-learn retrying pynndescent\n",
      "Failed to build hdbscan\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
